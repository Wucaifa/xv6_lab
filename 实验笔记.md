# xv6笔记

## Lab7: Multithreading

### Uthread: switching between threads

线程切换

给定的代码基础上实现用户级线程切换，因为是用户级线程，不需要设计用户栈和内核栈，用户页表和内核页表等等切换，所以本实验中只需要一个类似于`context`的结构，而不需要费尽心机的维护`trapframe`。

1. 定义存储上下文的结构体`tcontext`

   ```c
   // 用户线程的上下文结构体
   struct tcontext {
     uint64 ra;		// Return Address,通常是x1寄存器
     uint64 sp;		// Stack Pointer,通常是x2寄存器
   
     // callee-saved
     uint64 s0;
     uint64 s1;
     uint64 s2;
     uint64 s3;
     uint64 s4;
     uint64 s5;
     uint64 s6;
     uint64 s7;
     uint64 s8;
     uint64 s9;
     uint64 s10;
     uint64 s11;
   };
   
   ```

   `s0`-`s11`：对应RISC-V的`x8`-`x9`和`x18`-`x25`寄存器，这些寄存器在函数调用中必须由被调用者保存

   - 线程切换时保存当前线程的上下文
   - 恢复另一个线程的上下文
   - 初始化新线程的初始上下文

2. 修改`thread`结构体，添加`context`字段

   ```c
   struct thread {
     char            stack[STACK_SIZE];  /* the thread's stack */
     int             state;              /* FREE, RUNNING, RUNNABLE */
     struct tcontext context;            /* 用户进程上下文 */
   };
   ```

3. 模仿**kernel/swtch.S**，在kernel/uthread_switch.S中写入如下代码

   ```c
   .text
   
   /*
   * save the old thread's registers,
   * restore the new thread's registers.
   */
   
   .globl thread_switch
   thread_switch:
       /* YOUR CODE HERE */
       sd ra, 0(a0)
       sd sp, 8(a0)
       sd s0, 16(a0)
       sd s1, 24(a0)
       sd s2, 32(a0)
       sd s3, 40(a0)
       sd s4, 48(a0)
       sd s5, 56(a0)
       sd s6, 64(a0)
       sd s7, 72(a0)
       sd s8, 80(a0)
       sd s9, 88(a0)
       sd s10, 96(a0)
       sd s11, 104(a0)
   
       ld ra, 0(a1)
       ld sp, 8(a1)
       ld s0, 16(a1)
       ld s1, 24(a1)
       ld s2, 32(a1)
       ld s3, 40(a1)
       ld s4, 48(a1)
       ld s5, 56(a1)
       ld s6, 64(a1)
       ld s7, 72(a1)
       ld s8, 80(a1)
       ld s9, 88(a1)
       ld s10, 96(a1)
       ld s11, 104(a1)
       ret    /* return to ra */
   ```

   - 使用 `sd` (store doubleword) 指令将寄存器的值存储到内存。
   - 使用 `ld` (load doubleword) 指令从内存加载值到寄存器。

4. 修改`thread_scheduler`，添加线程切换语句

   ```c
   ...
   if (current_thread != next_thread) {         /* switch threads?  */
     ...
     /* YOUR CODE HERE */
     thread_switch((uint64)&t->context, (uint64)&current_thread->context);
   } else
     next_thread = 0;
   ```

5. 在`thread_create`中对`thread`结构体做一些初始化设定，主要是`ra`返回地址和`sp`栈指针，其他的都不重要

   ```c
   // YOUR CODE HERE
   t->context.ra = (uint64)func;                   // 设定函数返回地址
   t->context.sp = (uint64)t->stack + STACK_SIZE;  // 设定栈指针
   ```

### Using threads

设定了五个散列桶，根据键除以5的余数决定插入到哪一个散列桶中，插入方法是头插法。

问题：多线程插入如何保证数据安全性？

> 假设现在有两个线程T1和T2，两个线程都走到put函数，且假设两个线程中key%NBUCKET相等，即要插入同一个散列桶中。两个线程同时调用insert(key, value, &table[i], table[i])，insert是通过头插法实现的。如果先insert的线程还未返回另一个线程就开始insert，那么前面的数据会被覆盖

因此只需要对插入操作上锁即可

1. 为每个散列桶定义一个锁，将五个锁放在一个数组中，并进行初始化

   ```c
   pthread_mutex_t lock[NBUCKET] = { PTHREAD_MUTEX_INITIALIZER }; // 每个散列桶一把锁
   ```

2. 在`put`函数中对`insert`上锁

   ```c
   if(e){
       // update the existing key.
       e->value = value;
   } else {
       pthread_mutex_lock(&lock[i]);
       // the new is new.
       insert(key, value, &table[i], table[i]);
       pthread_mutex_unlock(&lock[i]);
   }
   ```

## Lab8: Locks

### Memory allocator

本实验完成的任务是为每个CPU都维护一个空闲列表，初始时将所有的空闲内存分配到某个CPU，此后各个CPU需要内存时，如果当前CPU的空闲列表上没有，则窃取其他CPU的。例如，所有的空闲内存初始分配到CPU0，当CPU1需要内存时就会窃取CPU0的，而使用完成后就挂在CPU1的空闲列表，此后CPU1再次需要内存时就可以从自己的空闲列表中取。

> 原先xv6代码是多个CPU使用kalloc造成了一个激烈的锁竞争，导致同一时刻只能有一个线程申请分配内存或释放。

1. 将`kmem`定义为一个数组，包含`NCPU`个元素，即每个CPU对应一个

   ```c
   struct {
     struct spinlock lock;
     struct run *freelist;
   } kmem[NCPU];
   ```

2. 修改`kinit`，为所有锁初始化以“kmem”开头的名称，该函数只会被一个CPU调用，`freerange`调用`kfree`将所有空闲内存挂在该CPU的空闲列表上

   ```c
   void
   kinit()
   {
     char lockname[8];
     for(int i = 0;i < NCPU; i++) {
       snprintf(lockname, sizeof(lockname), "kmem_%d", i);
       initlock(&kmem[i].lock, lockname);
     }
     freerange(end, (void*)PHYSTOP);
   }
   ```

3. 修改`kfree`，使用`cpuid()`和它返回的结果时必须关中断，请参考《XV6使用手册》第7.4节

   ```c
   void
   kfree(void *pa)
   {
     struct run *r;
   
     if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
       panic("kfree");
   
     // Fill with junk to catch dangling refs.
     memset(pa, 1, PGSIZE);
   
     r = (struct run*)pa;
   
     push_off();  // 关中断
     int id = cpuid();
     acquire(&kmem[id].lock);
     r->next = kmem[id].freelist;
     kmem[id].freelist = r;
     release(&kmem[id].lock);
     pop_off();  //开中断
   }
   ```

4. 修改`kalloc`，使得在当前CPU的空闲列表没有可分配内存时窃取其他内存的

   ```c
   void *
   kalloc(void)
   {
     struct run *r;
   
     push_off();// 关中断
     int id = cpuid();
     acquire(&kmem[id].lock);
     r = kmem[id].freelist;
     if(r)
       kmem[id].freelist = r->next;
     else {
       int antid;  // another id
       // 遍历所有CPU的空闲列表
       for(antid = 0; antid < NCPU; ++antid) {
         if(antid == id)
           continue;
         acquire(&kmem[antid].lock);
         r = kmem[antid].freelist;
         if(r) {
           kmem[antid].freelist = r->next;
           release(&kmem[antid].lock);
           break;
         }
         release(&kmem[antid].lock);
       }
     }
     release(&kmem[id].lock);
     pop_off();  //开中断
   
     if(r)
       memset((char*)r, 5, PGSIZE); // fill with junk
     return (void*)r;
   }
   ```

### Buffer cache

`bcache.lock`用来保护告诉缓存区的缓存块，多个进程不能同时操作磁盘缓存。

原先的设计中，当想要获取一个buff的时候，会给整个缓冲区上锁，当多个进程同时使用文件系统时，会发生严重的锁竞争。

方案：建立一个由blockno和dev到buf的哈希表，通过一个特定的哈希映射到不同的哈希桶，在每个桶上加锁，减小锁的颗粒度。当桶中空闲的buf不足时，从其他的桶中获取。

> 哈希映射的是每一个设备的不同序号，并不是直接映射每一个缓存块。初始状态下只有第一个桶有缓存块，当第一次查找非第一个桶的其他桶中的缓存的时候，此时这个桶只存有一个dummyhead，就不可能找到对应dev和blk的序号，此时一定是去第一个桶中利用最近最久未使用的方法逐出一个缓存块。因为每一个没有出现过的缓存库新的添加过程都是先逐出一个在第一个桶中从未被使用的块，然后添加到对应的key链表中。这个过程不仅不会打乱哈希映射，反而会使预想的哈希映射逐步实现。

1. 定义哈希桶结构，并在`bcache`中删除全局缓冲区链表，改为使用素数个散列桶

   ```c
   #define NBUCKET 13
   #define HASH(id) (id % NBUCKET)
   
   struct hashbuf {
     struct buf head;       // 头节点
     struct spinlock lock;  // 锁
   };
   
   struct {
     struct buf buf[NBUF];
     struct hashbuf buckets[NBUCKET];  // 散列桶
   } bcache;
   ```

2. 在`binit`中：

   （1）初始化散列桶的锁，

   （2）将所有散列桶的`head->prev`、`head->next`都指向自身表示为空，

   （3）将所有的缓冲区挂载到`bucket[0]`桶上，

   代码如下：

   ```c
   void
   binit(void) {
     struct buf* b;
     char lockname[16];
   
     for(int i = 0; i < NBUCKET; ++i) {
       // 初始化散列桶的自旋锁
       snprintf(lockname, sizeof(lockname), "bcache_%d", i);
       initlock(&bcache.buckets[i].lock, lockname);
   
       // 初始化散列桶的头节点
       bcache.buckets[i].head.prev = &bcache.buckets[i].head;
       bcache.buckets[i].head.next = &bcache.buckets[i].head;
     }
   
     // Create linked list of buffers
     for(b = bcache.buf; b < bcache.buf + NBUF; b++) {
       // 利用头插法初始化缓冲区列表,全部放到散列桶0上
       b->next = bcache.buckets[0].head.next;
       b->prev = &bcache.buckets[0].head;
       initsleeplock(&b->lock, "buffer");
       bcache.buckets[0].head.next->prev = b;
       bcache.buckets[0].head.next = b;
     }
   }
   ```

3. 在**buf.h**中增加新字段`timestamp`，这里来理解一下这个字段的用途：在原始方案中，每次`brelse`都将**被释放的缓冲区挂载到链表头**，禀明这个缓冲区最近刚刚被使用过，在`bget`中**分配时从链表尾向前查找**，这样符合条件的第一个就是最久未使用的。而在提示中建议使用时间戳作为LRU判定的法则，这样我们就无需在`brelse`中进行头插法更改结点位置

   ```c
   struct buf {
     ...
     ...
     uint timestamp;  // 时间戳
   };
   ```

4. 更改`brelse`，不再获取全局锁

   ```c
   void
   brelse(struct buf* b) {
     if(!holdingsleep(&b->lock))
       panic("brelse");
   
     int bid = HASH(b->blockno);
   
     releasesleep(&b->lock);
   
     acquire(&bcache.buckets[bid].lock);
     b->refcnt--;
   
     // 更新时间戳
     // 由于LRU改为使用时间戳判定，不再需要头插法
     acquire(&tickslock);
     b->timestamp = ticks;
     release(&tickslock);
   
     release(&bcache.buckets[bid].lock);
   }
   ```

5. 更改`bget`，当没有找到指定的缓冲区时进行分配，分配方式是优先从当前列表遍历，找到一个没有引用且`timestamp`最小的缓冲区，如果没有就申请下一个桶的锁，并遍历该桶，找到后将该缓冲区从原来的桶移动到当前桶中，最多将所有桶都遍历完。在代码中要注意锁的释放

   ```c
   static struct buf*
   bget(uint dev, uint blockno) {
     struct buf* b;
   
     int bid = HASH(blockno);
     acquire(&bcache.buckets[bid].lock);
   
     // Is the block already cached?
     for(b = bcache.buckets[bid].head.next; b != &bcache.buckets[bid].head; b = b->next) {
       if(b->dev == dev && b->blockno == blockno) {
         b->refcnt++;
   
         // 记录使用时间戳
         acquire(&tickslock);
         b->timestamp = ticks;
         release(&tickslock);
   
         release(&bcache.buckets[bid].lock);
         acquiresleep(&b->lock);
         return b;
       }
     }
   
     // Not cached.
     b = 0;
     struct buf* tmp;
   
     // Recycle the least recently used (LRU) unused buffer.
     // 从当前散列桶开始查找
     for(int i = bid, cycle = 0; cycle != NBUCKET; i = (i + 1) % NBUCKET) {
       ++cycle;
       // 如果遍历到当前散列桶，则不重新获取锁
       if(i != bid) {
         if(!holding(&bcache.buckets[i].lock))
           acquire(&bcache.buckets[i].lock);
         else
           continue;
       }
   
       for(tmp = bcache.buckets[i].head.next; tmp != &bcache.buckets[i].head; tmp = tmp->next)
         // 使用时间戳进行LRU算法，而不是根据结点在链表中的位置
         if(tmp->refcnt == 0 && (b == 0 || tmp->timestamp < b->timestamp))
           b = tmp;
   
       if(b) {
         // 如果是从其他散列桶窃取的，则将其以头插法插入到当前桶
         if(i != bid) {
           b->next->prev = b->prev;
           b->prev->next = b->next;
           release(&bcache.buckets[i].lock);
   
           b->next = bcache.buckets[bid].head.next;
           b->prev = &bcache.buckets[bid].head;
           bcache.buckets[bid].head.next->prev = b;
           bcache.buckets[bid].head.next = b;
         }
   
         b->dev = dev;
         b->blockno = blockno;
         b->valid = 0;
         b->refcnt = 1;
   
         acquire(&tickslock);
         b->timestamp = ticks;
         release(&tickslock);
   
         release(&bcache.buckets[bid].lock);
         acquiresleep(&b->lock);
         return b;
       } else {
         // 在当前散列桶中未找到，则直接释放锁
         if(i != bid)
           release(&bcache.buckets[i].lock);
       }
     }
   
     panic("bget: no buffers");
   }
   ```

6. 最后将末尾的两个小函数也改一下

   ```c
   void
   bpin(struct buf* b) {
     int bid = HASH(b->blockno);
     acquire(&bcache.buckets[bid].lock);
     b->refcnt++;
     release(&bcache.buckets[bid].lock);
   }
   
   void
   bunpin(struct buf* b) {
     int bid = HASH(b->blockno);
     acquire(&bcache.buckets[bid].lock);
     b->refcnt--;
     release(&bcache.buckets[bid].lock);
   }
   ```

7. 注意点：

   - bget中重新分配可能要持有两个锁，如果桶a持有自己的锁，再申请桶b的锁，与此同时如果桶b持有自己的锁，再申请桶a的锁就会造成死锁！因此代码中使用了`if(!holding(&bcache.bucket[i].lock))`来进行检查。此外，代码优先从自己的桶中获取缓冲区，如果自身没有依次向后查找这样的方式也尽可能地避免了前面的情况。（顺序）
   - 在`bget`中搜索缓冲区并在找不到缓冲区时为该缓冲区分配条目必须是原子的！在提示中说`bget`如果未找到而进行分配的操作可以是串行化的，也就是说多个CPU中未找到，应当串行的执行分配，同时还应当避免死锁。于是在发现未命中（Not cached）后，我写了如下的代码（此时未删除`bcache.lock`）

   ```c
   // 前半部分查找缓冲区的代码
   // Not cached
   release(&bcache.buckets[bid].lock);
   acquire(&bcache.lock);
   acquire(&bcache.buckets[bid].lock);
   // 后半部分分配缓冲区的代码
   ```

   这段代码中先释放了散列桶的锁之后再重新获取，之所以这样做是为了让所有代码都保证申请锁的顺序：先获取整个缓冲区的大锁再获取散列桶的小锁，这样才能避免死锁。**但是这样做却破坏了程序执行的原子性**。

   在`release`桶的锁并重新`acquire`的这段时间，另一个CPU可能也以相同的参数调用了`bget`，也发现没有该缓冲区并想要执行分配。最终的结果是一个磁盘块对应了两个缓冲区，破坏了最重要的不变量，即每个块最多缓存一个副本。这样会导致`usertests`中的`manywrites`测试报错：*panic: freeing free block*

<img width="771" height="190" alt="image" src="https://github.com/user-attachments/assets/3776858c-a2fe-45da-b396-1aeb9037a547" />


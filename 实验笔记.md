# xv6笔记

## Lab7: Multithreading

### Uthread: switching between threads

线程切换

给定的代码基础上实现用户级线程切换，因为是用户级线程，不需要设计用户栈和内核栈，用户页表和内核页表等等切换，所以本实验中只需要一个类似于`context`的结构，而不需要费尽心机的维护`trapframe`。

1. 定义存储上下文的结构体`tcontext`

   ```c
   // 用户线程的上下文结构体
   struct tcontext {
     uint64 ra;		// Return Address,通常是x1寄存器
     uint64 sp;		// Stack Pointer,通常是x2寄存器
   
     // callee-saved
     uint64 s0;
     uint64 s1;
     uint64 s2;
     uint64 s3;
     uint64 s4;
     uint64 s5;
     uint64 s6;
     uint64 s7;
     uint64 s8;
     uint64 s9;
     uint64 s10;
     uint64 s11;
   };
   
   ```

   `s0`-`s11`：对应RISC-V的`x8`-`x9`和`x18`-`x25`寄存器，这些寄存器在函数调用中必须由被调用者保存

   - 线程切换时保存当前线程的上下文
   - 恢复另一个线程的上下文
   - 初始化新线程的初始上下文

2. 修改`thread`结构体，添加`context`字段

   ```c
   struct thread {
     char            stack[STACK_SIZE];  /* the thread's stack */
     int             state;              /* FREE, RUNNING, RUNNABLE */
     struct tcontext context;            /* 用户进程上下文 */
   };
   ```

3. 模仿**kernel/swtch.S**，在kernel/uthread_switch.S中写入如下代码

   ```c
   .text
   
   /*
   * save the old thread's registers,
   * restore the new thread's registers.
   */
   
   .globl thread_switch
   thread_switch:
       /* YOUR CODE HERE */
       sd ra, 0(a0)
       sd sp, 8(a0)
       sd s0, 16(a0)
       sd s1, 24(a0)
       sd s2, 32(a0)
       sd s3, 40(a0)
       sd s4, 48(a0)
       sd s5, 56(a0)
       sd s6, 64(a0)
       sd s7, 72(a0)
       sd s8, 80(a0)
       sd s9, 88(a0)
       sd s10, 96(a0)
       sd s11, 104(a0)
   
       ld ra, 0(a1)
       ld sp, 8(a1)
       ld s0, 16(a1)
       ld s1, 24(a1)
       ld s2, 32(a1)
       ld s3, 40(a1)
       ld s4, 48(a1)
       ld s5, 56(a1)
       ld s6, 64(a1)
       ld s7, 72(a1)
       ld s8, 80(a1)
       ld s9, 88(a1)
       ld s10, 96(a1)
       ld s11, 104(a1)
       ret    /* return to ra */
   ```

   - 使用 `sd` (store doubleword) 指令将寄存器的值存储到内存。
   - 使用 `ld` (load doubleword) 指令从内存加载值到寄存器。

4. 修改`thread_scheduler`，添加线程切换语句

   ```c
   ...
   if (current_thread != next_thread) {         /* switch threads?  */
     ...
     /* YOUR CODE HERE */
     thread_switch((uint64)&t->context, (uint64)&current_thread->context);
   } else
     next_thread = 0;
   ```

5. 在`thread_create`中对`thread`结构体做一些初始化设定，主要是`ra`返回地址和`sp`栈指针，其他的都不重要

   ```c
   // YOUR CODE HERE
   t->context.ra = (uint64)func;                   // 设定函数返回地址
   t->context.sp = (uint64)t->stack + STACK_SIZE;  // 设定栈指针
   ```

### Using threads

设定了五个散列桶，根据键除以5的余数决定插入到哪一个散列桶中，插入方法是头插法。

问题：多线程插入如何保证数据安全性？

> 假设现在有两个线程T1和T2，两个线程都走到put函数，且假设两个线程中key%NBUCKET相等，即要插入同一个散列桶中。两个线程同时调用insert(key, value, &table[i], table[i])，insert是通过头插法实现的。如果先insert的线程还未返回另一个线程就开始insert，那么前面的数据会被覆盖

因此只需要对插入操作上锁即可

1. 为每个散列桶定义一个锁，将五个锁放在一个数组中，并进行初始化

   ```c
   pthread_mutex_t lock[NBUCKET] = { PTHREAD_MUTEX_INITIALIZER }; // 每个散列桶一把锁
   ```

2. 在`put`函数中对`insert`上锁

   ```c
   if(e){
       // update the existing key.
       e->value = value;
   } else {
       pthread_mutex_lock(&lock[i]);
       // the new is new.
       insert(key, value, &table[i], table[i]);
       pthread_mutex_unlock(&lock[i]);
   }
   ```

## Lab8: Locks

### Memory allocator

本实验完成的任务是为每个CPU都维护一个空闲列表，初始时将所有的空闲内存分配到某个CPU，此后各个CPU需要内存时，如果当前CPU的空闲列表上没有，则窃取其他CPU的。例如，所有的空闲内存初始分配到CPU0，当CPU1需要内存时就会窃取CPU0的，而使用完成后就挂在CPU1的空闲列表，此后CPU1再次需要内存时就可以从自己的空闲列表中取。

> 原先xv6代码是多个CPU使用kalloc造成了一个激烈的锁竞争，导致同一时刻只能有一个线程申请分配内存或释放。

1. 将`kmem`定义为一个数组，包含`NCPU`个元素，即每个CPU对应一个

   ```c
   struct {
     struct spinlock lock;
     struct run *freelist;
   } kmem[NCPU];
   ```

2. 修改`kinit`，为所有锁初始化以“kmem”开头的名称，该函数只会被一个CPU调用，`freerange`调用`kfree`将所有空闲内存挂在该CPU的空闲列表上

   ```c
   void
   kinit()
   {
     char lockname[8];
     for(int i = 0;i < NCPU; i++) {
       snprintf(lockname, sizeof(lockname), "kmem_%d", i);
       initlock(&kmem[i].lock, lockname);
     }
     freerange(end, (void*)PHYSTOP);
   }
   ```

3. 修改`kfree`，使用`cpuid()`和它返回的结果时必须关中断，请参考《XV6使用手册》第7.4节

   ```c
   void
   kfree(void *pa)
   {
     struct run *r;
   
     if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
       panic("kfree");
   
     // Fill with junk to catch dangling refs.
     memset(pa, 1, PGSIZE);
   
     r = (struct run*)pa;
   
     push_off();  // 关中断
     int id = cpuid();
     acquire(&kmem[id].lock);
     r->next = kmem[id].freelist;
     kmem[id].freelist = r;
     release(&kmem[id].lock);
     pop_off();  //开中断
   }
   ```

4. 修改`kalloc`，使得在当前CPU的空闲列表没有可分配内存时窃取其他内存的

   ```c
   void *
   kalloc(void)
   {
     struct run *r;
   
     push_off();// 关中断
     int id = cpuid();
     acquire(&kmem[id].lock);
     r = kmem[id].freelist;
     if(r)
       kmem[id].freelist = r->next;
     else {
       int antid;  // another id
       // 遍历所有CPU的空闲列表
       for(antid = 0; antid < NCPU; ++antid) {
         if(antid == id)
           continue;
         acquire(&kmem[antid].lock);
         r = kmem[antid].freelist;
         if(r) {
           kmem[antid].freelist = r->next;
           release(&kmem[antid].lock);
           break;
         }
         release(&kmem[antid].lock);
       }
     }
     release(&kmem[id].lock);
     pop_off();  //开中断
   
     if(r)
       memset((char*)r, 5, PGSIZE); // fill with junk
     return (void*)r;
   }
   ```

### Buffer cache

`bcache.lock`用来保护告诉缓存区的缓存块，多个进程不能同时操作磁盘缓存。

原先的设计中，当想要获取一个buff的时候，会给整个缓冲区上锁，当多个进程同时使用文件系统时，会发生严重的锁竞争。

方案：建立一个由blockno和dev到buf的哈希表，通过一个特定的哈希映射到不同的哈希桶，在每个桶上加锁，减小锁的颗粒度。当桶中空闲的buf不足时，从其他的桶中获取。

> 哈希映射的是每一个设备的不同序号，并不是直接映射每一个缓存块。初始状态下只有第一个桶有缓存块，当第一次查找非第一个桶的其他桶中的缓存的时候，此时这个桶只存有一个dummyhead，就不可能找到对应dev和blk的序号，此时一定是去第一个桶中利用最近最久未使用的方法逐出一个缓存块。因为每一个没有出现过的缓存库新的添加过程都是先逐出一个在第一个桶中从未被使用的块，然后添加到对应的key链表中。这个过程不仅不会打乱哈希映射，反而会使预想的哈希映射逐步实现。

1. 定义哈希桶结构，并在`bcache`中删除全局缓冲区链表，改为使用素数个散列桶

   ```c
   #define NBUCKET 13
   #define HASH(id) (id % NBUCKET)
   
   struct hashbuf {
     struct buf head;       // 头节点
     struct spinlock lock;  // 锁
   };
   
   struct {
     struct buf buf[NBUF];
     struct hashbuf buckets[NBUCKET];  // 散列桶
   } bcache;
   ```

2. 在`binit`中：

   （1）初始化散列桶的锁，

   （2）将所有散列桶的`head->prev`、`head->next`都指向自身表示为空，

   （3）将所有的缓冲区挂载到`bucket[0]`桶上，

   代码如下：

   ```c
   void
   binit(void) {
     struct buf* b;
     char lockname[16];
   
     for(int i = 0; i < NBUCKET; ++i) {
       // 初始化散列桶的自旋锁
       snprintf(lockname, sizeof(lockname), "bcache_%d", i);
       initlock(&bcache.buckets[i].lock, lockname);
   
       // 初始化散列桶的头节点
       bcache.buckets[i].head.prev = &bcache.buckets[i].head;
       bcache.buckets[i].head.next = &bcache.buckets[i].head;
     }
   
     // Create linked list of buffers
     for(b = bcache.buf; b < bcache.buf + NBUF; b++) {
       // 利用头插法初始化缓冲区列表,全部放到散列桶0上
       b->next = bcache.buckets[0].head.next;
       b->prev = &bcache.buckets[0].head;
       initsleeplock(&b->lock, "buffer");
       bcache.buckets[0].head.next->prev = b;
       bcache.buckets[0].head.next = b;
     }
   }
   ```

3. 在**buf.h**中增加新字段`timestamp`，这里来理解一下这个字段的用途：在原始方案中，每次`brelse`都将**被释放的缓冲区挂载到链表头**，禀明这个缓冲区最近刚刚被使用过，在`bget`中**分配时从链表尾向前查找**，这样符合条件的第一个就是最久未使用的。而在提示中建议使用时间戳作为LRU判定的法则，这样我们就无需在`brelse`中进行头插法更改结点位置

   ```c
   struct buf {
     ...
     ...
     uint timestamp;  // 时间戳
   };
   ```

4. 更改`brelse`，不再获取全局锁

   ```c
   void
   brelse(struct buf* b) {
     if(!holdingsleep(&b->lock))
       panic("brelse");
   
     int bid = HASH(b->blockno);
   
     releasesleep(&b->lock);
   
     acquire(&bcache.buckets[bid].lock);
     b->refcnt--;
   
     // 更新时间戳
     // 由于LRU改为使用时间戳判定，不再需要头插法
     acquire(&tickslock);
     b->timestamp = ticks;
     release(&tickslock);
   
     release(&bcache.buckets[bid].lock);
   }
   ```

5. 更改`bget`，当没有找到指定的缓冲区时进行分配，分配方式是优先从当前列表遍历，找到一个没有引用且`timestamp`最小的缓冲区，如果没有就申请下一个桶的锁，并遍历该桶，找到后将该缓冲区从原来的桶移动到当前桶中，最多将所有桶都遍历完。在代码中要注意锁的释放

   ```c
   static struct buf*
   bget(uint dev, uint blockno) {
     struct buf* b;
   
     int bid = HASH(blockno);
     acquire(&bcache.buckets[bid].lock);
   
     // Is the block already cached?
     for(b = bcache.buckets[bid].head.next; b != &bcache.buckets[bid].head; b = b->next) {
       if(b->dev == dev && b->blockno == blockno) {
         b->refcnt++;
   
         // 记录使用时间戳
         acquire(&tickslock);
         b->timestamp = ticks;
         release(&tickslock);
   
         release(&bcache.buckets[bid].lock);
         acquiresleep(&b->lock);
         return b;
       }
     }
   
     // Not cached.
     b = 0;
     struct buf* tmp;
   
     // Recycle the least recently used (LRU) unused buffer.
     // 从当前散列桶开始查找
     for(int i = bid, cycle = 0; cycle != NBUCKET; i = (i + 1) % NBUCKET) {
       ++cycle;
       // 如果遍历到当前散列桶，则不重新获取锁
       if(i != bid) {
         if(!holding(&bcache.buckets[i].lock))
           acquire(&bcache.buckets[i].lock);
         else
           continue;
       }
   
       for(tmp = bcache.buckets[i].head.next; tmp != &bcache.buckets[i].head; tmp = tmp->next)
         // 使用时间戳进行LRU算法，而不是根据结点在链表中的位置
         if(tmp->refcnt == 0 && (b == 0 || tmp->timestamp < b->timestamp))
           b = tmp;
   
       if(b) {
         // 如果是从其他散列桶窃取的，则将其以头插法插入到当前桶
         if(i != bid) {
           b->next->prev = b->prev;
           b->prev->next = b->next;
           release(&bcache.buckets[i].lock);
   
           b->next = bcache.buckets[bid].head.next;
           b->prev = &bcache.buckets[bid].head;
           bcache.buckets[bid].head.next->prev = b;
           bcache.buckets[bid].head.next = b;
         }
   
         b->dev = dev;
         b->blockno = blockno;
         b->valid = 0;
         b->refcnt = 1;
   
         acquire(&tickslock);
         b->timestamp = ticks;
         release(&tickslock);
   
         release(&bcache.buckets[bid].lock);
         acquiresleep(&b->lock);
         return b;
       } else {
         // 在当前散列桶中未找到，则直接释放锁
         if(i != bid)
           release(&bcache.buckets[i].lock);
       }
     }
   
     panic("bget: no buffers");
   }
   ```

6. 最后将末尾的两个小函数也改一下

   ```c
   void
   bpin(struct buf* b) {
     int bid = HASH(b->blockno);
     acquire(&bcache.buckets[bid].lock);
     b->refcnt++;
     release(&bcache.buckets[bid].lock);
   }
   
   void
   bunpin(struct buf* b) {
     int bid = HASH(b->blockno);
     acquire(&bcache.buckets[bid].lock);
     b->refcnt--;
     release(&bcache.buckets[bid].lock);
   }
   ```

7. 注意点：

   - bget中重新分配可能要持有两个锁，如果桶a持有自己的锁，再申请桶b的锁，与此同时如果桶b持有自己的锁，再申请桶a的锁就会造成死锁！因此代码中使用了`if(!holding(&bcache.bucket[i].lock))`来进行检查。此外，代码优先从自己的桶中获取缓冲区，如果自身没有依次向后查找这样的方式也尽可能地避免了前面的情况。（顺序）
   - 在`bget`中搜索缓冲区并在找不到缓冲区时为该缓冲区分配条目必须是原子的！在提示中说`bget`如果未找到而进行分配的操作可以是串行化的，也就是说多个CPU中未找到，应当串行的执行分配，同时还应当避免死锁。于是在发现未命中（Not cached）后，我写了如下的代码（此时未删除`bcache.lock`）

   ```c
   // 前半部分查找缓冲区的代码
   // Not cached
   release(&bcache.buckets[bid].lock);
   acquire(&bcache.lock);
   acquire(&bcache.buckets[bid].lock);
   // 后半部分分配缓冲区的代码
   ```

   这段代码中先释放了散列桶的锁之后再重新获取，之所以这样做是为了让所有代码都保证申请锁的顺序：先获取整个缓冲区的大锁再获取散列桶的小锁，这样才能避免死锁。**但是这样做却破坏了程序执行的原子性**。

   在`release`桶的锁并重新`acquire`的这段时间，另一个CPU可能也以相同的参数调用了`bget`，也发现没有该缓冲区并想要执行分配。最终的结果是一个磁盘块对应了两个缓冲区，破坏了最重要的不变量，即每个块最多缓存一个副本。这样会导致`usertests`中的`manywrites`测试报错：*panic: freeing free block*


## Lab9: file system

### Large files

增加xv6文件的最大大小。目前，xv6文件限制为268个块或`268*BSIZE`字节（在xv6中`BSIZE`为1024）。

此限制来自以下事实：一个xv6 inode包含12个“直接”块号和一个“间接”块号，“一级间接”块指一个最多可容纳256个块号的块，总共12+256=268个块。

> 目标：修改`bmap()`，以便除了直接块和一级间接块之外，它还实现二级间接块。你只需要有11个直接块，而不是12个，为你的新的二级间接块腾出空间；不允许更改磁盘inode的大小。`ip->addrs[]`的前11个元素应该是直接块；第12个应该是一个一级间接块（与当前的一样）；13号应该是你的新二级间接块。

xv6文件系统：

| 文件描述符（File descriptor）  |
| ------------------------------ |
| 路径名（Pathname）             |
| 目录（Directory）              |
| 索引结点（Inode）              |
| 日志（Logging）                |
| 缓冲区高速缓存（Buffer cache） |
| 磁盘（Disk）                   |

<img width="771" height="190" alt="image" src="https://github.com/user-attachments/assets/3776858c-a2fe-45da-b396-1aeb9037a547" />

1. 在fs.h中添加宏定义

   ```c
   #define NDIRECT 11
   #define NINDIRECT (BSIZE / sizeof(uint))
   #define NDINDIRECT ((BSIZE / sizeof(uint)) * (BSIZE / sizeof(uint)))
   #define MAXFILE (NDIRECT + NINDIRECT + NDINDIRECT)
   #define NADDR_PER_BLOCK (BSIZE / sizeof(uint))  // 一个块中的地址数量
   ```

   - `BSIZE`（磁盘块大小） / `uint`（指针大小，通常4字节），每个一级间接块可存储`1024`个数据块地址
   - **`NDINDIRECT` (二级间接块容量)**
   - **`MAXFILE` (最大文件块数)**
   - **`NADDR_PER_BLOCK` (每块地址数)**

2. 由于`NDIRECT`定义改变，其中一个直接块变为了二级间接块，需要修改inode结构体中`addrs`元素数量

   ```c
   // fs.h
   struct dinode {
     ...
     uint addrs[NDIRECT + 2];   // Data block addresses
   };
   
   // file.h
   struct inode {
     ...
     uint addrs[NDIRECT + 2]; // 0-10:直接索引 11:一级索引 12:二级索引
   };
   ```

3. 修改`bmap`支持二级索引

   ```c
   static uint
   bmap(struct inode *ip, uint bn)
   {
     uint addr, *a;
     struct buf *bp;
   
     if(bn < NDIRECT){
       ...
     }
     bn -= NDIRECT;
   
     if(bn < NINDIRECT){
       ...
     }
     bn -= NINDIRECT;
   
     // 二级间接块的情况
     if(bn < NDINDIRECT) {
       int level2_idx = bn / NADDR_PER_BLOCK;  // 要查找的块号位于二级间接块中的位置
       int level1_idx = bn % NADDR_PER_BLOCK;  // 要查找的块号位于一级间接块中的位置
       // 读出二级间接块
       if((addr = ip->addrs[NDIRECT + 1]) == 0)
         ip->addrs[NDIRECT + 1] = addr = balloc(ip->dev);
       bp = bread(ip->dev, addr);
       a = (uint*)bp->data;
   
       if((addr = a[level2_idx]) == 0) {
         a[level2_idx] = addr = balloc(ip->dev);
         // 更改了当前块的内容，标记以供后续写回磁盘
         log_write(bp);
       }
       brelse(bp);
   
       bp = bread(ip->dev, addr);
       a = (uint*)bp->data;
       if((addr = a[level1_idx]) == 0) {
         a[level1_idx] = addr = balloc(ip->dev);
         log_write(bp);
       }
       brelse(bp);
       return addr;
     }
   
     panic("bmap: out of range");
   }
   ```

4. 修改`itrunc`释放所有块

   ```c
   void
   itrunc(struct inode *ip)
   {
     int i, j;
     struct buf *bp;
     uint *a;
   
     for(i = 0; i < NDIRECT; i++){
       ...
     }
   
     if(ip->addrs[NDIRECT]){
       ...
     }
   
     struct buf* bp1;
     uint* a1;
     if(ip->addrs[NDIRECT + 1]) {
       bp = bread(ip->dev, ip->addrs[NDIRECT + 1]);
       a = (uint*)bp->data;
       for(i = 0; i < NADDR_PER_BLOCK; i++) {
         // 每个一级间接块的操作都类似于上面的
         // if(ip->addrs[NDIRECT])中的内容
         if(a[i]) {
           bp1 = bread(ip->dev, a[i]);
           a1 = (uint*)bp1->data;
           for(j = 0; j < NADDR_PER_BLOCK; j++) {
             if(a1[j])
               bfree(ip->dev, a1[j]);
           }
           brelse(bp1);
           bfree(ip->dev, a[i]);
         }
       }
       brelse(bp);
       bfree(ip->dev, ip->addrs[NDIRECT + 1]);
       ip->addrs[NDIRECT + 1] = 0;
     }
   
     ip->size = 0;
     iupdate(ip);
   }
   ```

### Symbolic links

> inode和dinode

磁盘上的inode由`struct dinode`（***kernel/fs.h***:32）定义。

```c
struct dinode {
  short type;           // 文件类型（普通文件、目录、设备等）
  short major;          // 主设备号（设备文件用）
  short minor;          // 次设备号（设备文件用）
  short nlink;          // 硬链接计数
  uint size;            // 文件大小（字节）
  uint addrs[NDIRECT+1]; // 数据块地址（直接+间接指针）记录保存文件内容的磁盘块的块号
};
```

`struct inode`（***kernel/file.h***:17）是磁盘上`struct dinode`的内存副本。

```c
struct inode {
  uint dev;              // 设备号
  uint inum;             // inode编号
  int ref;               // 引用计数
  struct sleeplock lock; // 睡眠锁
  int valid;             // inode内容是否有效
  
  // 从dinode拷贝的字段
  short type;
  short major;
  short minor;
  short nlink;
  uint size;
  uint addrs[NDIRECT+1];
};
```

- 只有当有C指针引用某个inode时，内核才会在内存中存储该inode。
- `iget`和`iput`函数分别获取和释放指向inode的指针，修改引用计数。

实现软链接机制symlink

- 硬链接：是原始文件的另一个目录条目，指向相同的inode和数据块。删除原始文件后，硬链接仍然可以访问数据
- 软链接：是一个特殊文件，包含指向另一个文件/目录的路径。原始文件删除后，软链接将失效。

**存储内容**：仅存储目标路径字符串（如`"/usr/bin/python"`）

1. 配置系统调用的常规操作，如在***user/usys.pl***、***user/user.h***中添加一个条目，在***kernel/syscall.c***、***kernel/syscall.h***中添加相关内容

2. 添加提示中的相关定义，`T_SYMLINK`以及`O_NOFOLLOW`

   ```c
   // fcntl.h
   #define O_NOFOLLOW 0x004
   // stat.h
   #define T_SYMLINK 4
   ```

3.  在***kernel/sysfile.c***中实现`sys_symlink`，这里需要注意的是`create`返回已加锁的inode，此外`iunlockput`既对inode解锁，还将其引用计数减1，计数为0时回收此inode

   - 当 `nlink == 0` 且 `ref == 0` 时，文件数据块才会被真正释放

   ```c
   uint64
   sys_symlink(void) {
     char target[MAXPATH], path[MAXPATH];
     struct inode* ip_path;
   
     if(argstr(0, target, MAXPATH) < 0 || argstr(1, path, MAXPATH) < 0) {
       return -1;
     }
   
     begin_op();
     // 分配一个inode结点，create返回锁定的inode
     ip_path = create(path, T_SYMLINK, 0, 0);
     if(ip_path == 0) {
       end_op();
       return -1;
     }
     // 向inode数据块中写入target路径
     if(writei(ip_path, 0, (uint64)target, 0, MAXPATH) < MAXPATH) {
       iunlockput(ip_path);
       end_op();
       return -1;
     }
   
     iunlockput(ip_path);
     end_op();
     return 0;
   }
   ```

4. 修改`sys_open`支持打开符号链接

   ```c
   uint64
   sys_open(void)
   {
     ...
   
     if(ip->type == T_DEVICE && (ip->major < 0 || ip->major >= NDEV)){
       ...
     }
   
     // 处理符号链接
     if(ip->type == T_SYMLINK && !(omode & O_NOFOLLOW)) {
       // 若符号链接指向的仍然是符号链接，则递归的跟随它
       // 直到找到真正指向的文件
       // 但深度不能超过MAX_SYMLINK_DEPTH
       for(int i = 0; i < MAX_SYMLINK_DEPTH; ++i) {
         // 读出符号链接指向的路径
         if(readi(ip, 0, (uint64)path, 0, MAXPATH) != MAXPATH) {
           iunlockput(ip);
           end_op();
           return -1;
         }
         iunlockput(ip);
         ip = namei(path);
         if(ip == 0) {
           end_op();
           return -1;
         }
         ilock(ip);
         if(ip->type != T_SYMLINK)
           break;
       }
       // 超过最大允许深度后仍然为符号链接，则返回错误
       if(ip->type == T_SYMLINK) {
         iunlockput(ip);
         end_op();
         return -1;
       }
     }
   
     if((f = filealloc()) == 0 || (fd = fdalloc(f)) < 0){
       ...
     }
   
     ...
     return fd;
   }
   ```

   
